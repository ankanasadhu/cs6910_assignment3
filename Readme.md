## Assignment 3 : Implementation of RNNs for transliteration

* The two folders 'attention_with_sequence_to_sequence' and 'vanilla_sequence_to_sequence' folders contain the source code for the implementation of different variants of RNN for a transliteration system.

* The 'vanilla_sequence_to_sequence' folder contains the code for RNN, LSTM and GRU sequence models. All the three models were trained on 'dakshina' dataset and the best model with the best hyperparameter configuration was obtained using wandb sweeps.

* The 'attention_with_sequence_to_sequence' folder contains the GRU model with attention. The best hyperparameter configuration for the attention based model was obtained using wandb sweeps.

## Language Used
* We have used hindi language for the transliteration.

## Wandb Report

* https://wandb.ai/ankanasadhu98/Deep%20Learning-RNN/reports/Assignment-3--VmlldzoxOTUxMDE3



